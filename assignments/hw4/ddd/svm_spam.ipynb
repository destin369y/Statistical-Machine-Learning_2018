{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 10 1e-05 0.76\n",
      "0.3 10 3e-05 0.76\n",
      "0.3 10 0.0001 0.76\n",
      "0.3 10 0.0003 0.76\n",
      "0.3 10 0.001 0.76\n",
      "0.3 10 0.003 0.76\n",
      "0.3 10 0.01 0.76\n",
      "0.3 30 1e-05 0.76\n",
      "0.3 30 3e-05 0.76\n",
      "0.3 30 0.0001 0.76\n",
      "0.3 30 0.0003 0.76\n",
      "0.3 30 0.001 0.76\n",
      "0.3 30 0.003 0.76\n",
      "0.3 30 0.01 0.76\n",
      "0.3 100 1e-05 0.76\n",
      "0.3 100 3e-05 0.76\n",
      "0.3 100 0.0001 0.76\n",
      "0.3 100 0.0003 0.76\n",
      "0.3 100 0.001 0.76\n",
      "0.3 100 0.003 0.76\n",
      "0.3 100 0.01 0.76\n",
      "0.3 300 1e-05 0.76\n",
      "0.3 300 3e-05 0.76\n",
      "0.3 300 0.0001 0.76\n",
      "0.3 300 0.0003 0.76\n",
      "0.3 300 0.001 0.76\n",
      "0.3 300 0.003 0.76\n",
      "0.3 300 0.01 0.76\n",
      "0.3 1000 1e-05 0.76\n",
      "0.3 1000 3e-05 0.76\n",
      "0.3 1000 0.0001 0.76\n",
      "0.3 1000 0.0003 0.76\n",
      "0.3 1000 0.001 0.76\n",
      "0.3 1000 0.003 0.76\n",
      "0.3 1000 0.01 0.76\n",
      "0.3 3000 1e-05 0.76\n",
      "0.3 3000 3e-05 0.76\n",
      "0.3 3000 0.0001 0.76\n",
      "0.3 3000 0.0003 0.76\n",
      "0.3 3000 0.001 0.76\n",
      "0.3 3000 0.003 0.76\n",
      "0.3 3000 0.01 0.76\n",
      "0.3 10000 1e-05 0.76\n",
      "0.3 10000 3e-05 0.76\n",
      "0.3 10000 0.0001 0.76\n",
      "0.3 10000 0.0003 0.76\n",
      "0.3 10000 0.001 0.76\n",
      "0.3 10000 0.003 0.76\n",
      "0.3 10000 0.01 0.76\n",
      "1 10 1e-05 0.78\n",
      "1 10 3e-05 0.7775\n",
      "1 10 0.0001 0.7775\n",
      "1 10 0.0003 0.78\n",
      "1 10 0.001 0.78\n",
      "1 10 0.003 0.78\n",
      "1 10 0.01 0.78\n",
      "1 30 1e-05 0.78\n",
      "1 30 3e-05 0.78\n",
      "1 30 0.0001 0.78\n",
      "1 30 0.0003 0.78\n",
      "1 30 0.001 0.78\n",
      "1 30 0.003 0.78\n",
      "1 30 0.01 0.78\n",
      "1 100 1e-05 0.78\n",
      "1 100 3e-05 0.78\n",
      "1 100 0.0001 0.78\n",
      "1 100 0.0003 0.78\n",
      "1 100 0.001 0.78\n",
      "1 100 0.003 0.78\n",
      "1 100 0.01 0.78\n",
      "1 300 1e-05 0.78\n",
      "1 300 3e-05 0.78\n",
      "1 300 0.0001 0.78\n",
      "1 300 0.0003 0.78\n",
      "1 300 0.001 0.78\n",
      "1 300 0.003 0.78\n",
      "1 300 0.01 0.7825\n",
      "1 1000 1e-05 0.7825\n",
      "1 1000 3e-05 0.7825\n",
      "1 1000 0.0001 0.7825\n",
      "1 1000 0.0003 0.7825\n",
      "1 1000 0.001 0.7825\n",
      "1 1000 0.003 0.7825\n",
      "1 1000 0.01 0.7825\n",
      "1 3000 1e-05 0.7825\n",
      "1 3000 3e-05 0.7825\n",
      "1 3000 0.0001 0.7825\n",
      "1 3000 0.0003 0.7825\n",
      "1 3000 0.001 0.7825\n",
      "1 3000 0.003 0.7825\n",
      "1 3000 0.01 0.7825\n",
      "1 10000 1e-05 0.7825\n",
      "1 10000 3e-05 0.7825\n",
      "1 10000 0.0001 0.7825\n",
      "1 10000 0.0003 0.7825\n",
      "1 10000 0.001 0.7825\n",
      "1 10000 0.003 0.7825\n",
      "1 10000 0.01 0.7825\n",
      "3 10 1e-05 0.5975\n",
      "3 10 3e-05 0.72\n",
      "3 10 0.0001 0.895\n",
      "3 10 0.0003 0.895\n",
      "3 10 0.001 0.8975\n",
      "3 10 0.003 0.8925\n",
      "3 10 0.01 0.8975\n",
      "3 30 1e-05 0.8975\n",
      "3 30 3e-05 0.8975\n",
      "3 30 0.0001 0.8975\n",
      "3 30 0.0003 0.8975\n",
      "3 30 0.001 0.89\n",
      "3 30 0.003 0.8975\n",
      "3 30 0.01 0.8975\n",
      "3 100 1e-05 0.9\n",
      "3 100 3e-05 0.9\n",
      "3 100 0.0001 0.9\n",
      "3 100 0.0003 0.9025\n",
      "3 100 0.001 0.8975\n",
      "3 100 0.003 0.8975\n",
      "3 100 0.01 0.875\n",
      "3 300 1e-05 0.92\n",
      "3 300 3e-05 0.895\n",
      "3 300 0.0001 0.895\n",
      "3 300 0.0003 0.8925\n",
      "3 300 0.001 0.8925\n",
      "3 300 0.003 0.91\n",
      "3 300 0.01 0.885\n",
      "3 1000 1e-05 0.89\n",
      "3 1000 3e-05 0.895\n",
      "3 1000 0.0001 0.895\n",
      "3 1000 0.0003 0.91\n",
      "3 1000 0.001 0.8975\n",
      "3 1000 0.003 0.89\n",
      "3 1000 0.01 0.9225\n",
      "3 3000 1e-05 0.9225\n",
      "3 3000 3e-05 0.9125\n",
      "3 3000 0.0001 0.91\n",
      "3 3000 0.0003 0.9075\n",
      "3 3000 0.001 0.9175\n",
      "3 3000 0.003 0.91\n",
      "3 3000 0.01 0.905\n",
      "3 10000 1e-05 0.9025\n",
      "3 10000 3e-05 0.9\n",
      "3 10000 0.0001 0.9\n",
      "3 10000 0.0003 0.8925\n",
      "3 10000 0.001 0.9\n",
      "3 10000 0.003 0.9175\n",
      "3 10000 0.01 0.895\n",
      "10 10 1e-05 0.81\n",
      "10 10 3e-05 0.9175\n",
      "10 10 0.0001 0.9275\n",
      "10 10 0.0003 0.9525\n",
      "10 10 0.001 0.955\n",
      "10 10 0.003 0.9525\n",
      "10 10 0.01 0.8375\n",
      "10 30 1e-05 0.935\n",
      "10 30 3e-05 0.9525\n",
      "10 30 0.0001 0.9525\n",
      "10 30 0.0003 0.955\n",
      "10 30 0.001 0.96\n",
      "10 30 0.003 0.9725\n",
      "10 30 0.01 0.955\n",
      "10 100 1e-05 0.955\n",
      "10 100 3e-05 0.955\n",
      "10 100 0.0001 0.9525\n",
      "10 100 0.0003 0.955\n",
      "10 100 0.001 0.96\n",
      "10 100 0.003 0.975\n",
      "10 100 0.01 0.7775\n",
      "10 300 1e-05 0.8875\n",
      "10 300 3e-05 0.9575\n",
      "10 300 0.0001 0.955\n",
      "10 300 0.0003 0.96\n",
      "10 300 0.001 0.96\n",
      "10 300 0.003 0.9625\n",
      "10 300 0.01 0.9575\n",
      "10 1000 1e-05 0.955\n",
      "10 1000 3e-05 0.95\n",
      "10 1000 0.0001 0.95\n",
      "10 1000 0.0003 0.96\n",
      "10 1000 0.001 0.9625\n",
      "10 1000 0.003 0.95\n",
      "10 1000 0.01 0.7225\n",
      "10 3000 1e-05 0.9375\n",
      "10 3000 3e-05 0.95\n",
      "10 3000 0.0001 0.95\n",
      "10 3000 0.0003 0.955\n",
      "10 3000 0.001 0.9625\n",
      "10 3000 0.003 0.9675\n",
      "10 3000 0.01 0.8575\n",
      "10 10000 1e-05 0.9525\n",
      "10 10000 3e-05 0.955\n",
      "10 10000 0.0001 0.9575\n",
      "10 10000 0.0003 0.9575\n",
      "10 10000 0.001 0.965\n",
      "10 10000 0.003 0.96\n",
      "10 10000 0.01 0.9475\n",
      "30 10 1e-05 0.8225\n",
      "30 10 3e-05 0.945\n",
      "30 10 0.0001 0.925\n",
      "30 10 0.0003 0.8975\n",
      "30 10 0.001 0.8725\n",
      "30 10 0.003 0.8575\n",
      "30 10 0.01 0.8875\n",
      "30 30 1e-05 0.9525\n",
      "30 30 3e-05 0.96\n",
      "30 30 0.0001 0.96\n",
      "30 30 0.0003 0.96\n",
      "30 30 0.001 0.9575\n",
      "30 30 0.003 0.96\n",
      "30 30 0.01 0.7125\n",
      "30 100 1e-05 0.9325\n",
      "30 100 3e-05 0.9575\n",
      "30 100 0.0001 0.96\n",
      "30 100 0.0003 0.96\n",
      "30 100 0.001 0.9625\n",
      "30 100 0.003 0.885\n",
      "30 100 0.01 0.9575\n",
      "30 300 1e-05 0.96\n",
      "30 300 3e-05 0.955\n",
      "30 300 0.0001 0.96\n",
      "30 300 0.0003 0.96\n",
      "30 300 0.001 0.9625\n",
      "30 300 0.003 0.9\n",
      "30 300 0.01 0.8875\n",
      "30 1000 1e-05 0.945\n",
      "30 1000 3e-05 0.955\n",
      "30 1000 0.0001 0.96\n",
      "30 1000 0.0003 0.96\n",
      "30 1000 0.001 0.965\n",
      "30 1000 0.003 0.9375\n",
      "30 1000 0.01 0.9575\n",
      "30 3000 1e-05 0.96\n",
      "30 3000 3e-05 0.96\n",
      "30 3000 0.0001 0.96\n",
      "30 3000 0.0003 0.96\n",
      "30 3000 0.001 0.96\n",
      "30 3000 0.003 0.965\n",
      "30 3000 0.01 0.945\n",
      "30 10000 1e-05 0.955\n",
      "30 10000 3e-05 0.955\n",
      "30 10000 0.0001 0.96\n",
      "30 10000 0.0003 0.96\n",
      "30 10000 0.001 0.9625\n",
      "30 10000 0.003 0.8675\n",
      "30 10000 0.01 0.96\n",
      "100 10 1e-05 0.82\n",
      "100 10 3e-05 0.9325\n",
      "100 10 0.0001 0.9275\n",
      "100 10 0.0003 0.9325\n",
      "100 10 0.001 0.9425\n",
      "100 10 0.003 0.9525\n",
      "100 10 0.01 0.955\n",
      "100 30 1e-05 0.96\n",
      "100 30 3e-05 0.96\n",
      "100 30 0.0001 0.96\n",
      "100 30 0.0003 0.96\n",
      "100 30 0.001 0.9625\n",
      "100 30 0.003 0.865\n",
      "100 30 0.01 0.955\n",
      "100 100 1e-05 0.96\n",
      "100 100 3e-05 0.96\n",
      "100 100 0.0001 0.96\n",
      "100 100 0.0003 0.96\n",
      "100 100 0.001 0.9675\n",
      "100 100 0.003 0.965\n",
      "100 100 0.01 0.955\n",
      "100 300 1e-05 0.9525\n",
      "100 300 3e-05 0.9575\n",
      "100 300 0.0001 0.96\n",
      "100 300 0.0003 0.96\n",
      "100 300 0.001 0.9575\n",
      "100 300 0.003 0.7275\n",
      "100 300 0.01 0.955\n",
      "100 1000 1e-05 0.955\n",
      "100 1000 3e-05 0.955\n",
      "100 1000 0.0001 0.96\n",
      "100 1000 0.0003 0.96\n",
      "100 1000 0.001 0.9675\n",
      "100 1000 0.003 0.95\n",
      "100 1000 0.01 0.9525\n",
      "100 3000 1e-05 0.9575\n",
      "100 3000 3e-05 0.9575\n",
      "100 3000 0.0001 0.96\n",
      "100 3000 0.0003 0.96\n",
      "100 3000 0.001 0.965\n",
      "100 3000 0.003 0.935\n",
      "100 3000 0.01 0.87\n",
      "100 10000 1e-05 0.9375\n",
      "100 10000 3e-05 0.96\n",
      "100 10000 0.0001 0.96\n",
      "100 10000 0.0003 0.96\n",
      "100 10000 0.001 0.9625\n",
      "100 10000 0.003 0.92\n",
      "100 10000 0.01 0.935\n",
      "300 10 1e-05 0.825\n",
      "300 10 3e-05 0.92\n",
      "300 10 0.0001 0.9075\n",
      "300 10 0.0003 0.945\n",
      "300 10 0.001 0.96\n",
      "300 10 0.003 0.88\n",
      "300 10 0.01 0.9425\n",
      "300 30 1e-05 0.955\n",
      "300 30 3e-05 0.96\n",
      "300 30 0.0001 0.96\n",
      "300 30 0.0003 0.9575\n",
      "300 30 0.001 0.9575\n",
      "300 30 0.003 0.955\n",
      "300 30 0.01 0.9125\n",
      "300 100 1e-05 0.955\n",
      "300 100 3e-05 0.96\n",
      "300 100 0.0001 0.96\n",
      "300 100 0.0003 0.96\n",
      "300 100 0.001 0.965\n",
      "300 100 0.003 0.9625\n",
      "300 100 0.01 0.6625\n",
      "300 300 1e-05 0.92\n",
      "300 300 3e-05 0.9525\n",
      "300 300 0.0001 0.9575\n",
      "300 300 0.0003 0.96\n",
      "300 300 0.001 0.9675\n",
      "300 300 0.003 0.965\n",
      "300 300 0.01 0.9575\n",
      "300 1000 1e-05 0.96\n",
      "300 1000 3e-05 0.96\n",
      "300 1000 0.0001 0.96\n",
      "300 1000 0.0003 0.96\n",
      "300 1000 0.001 0.9625\n",
      "300 1000 0.003 0.9225\n",
      "300 1000 0.01 0.945\n",
      "300 3000 1e-05 0.9575\n",
      "300 3000 3e-05 0.96\n",
      "300 3000 0.0001 0.96\n",
      "300 3000 0.0003 0.96\n",
      "300 3000 0.001 0.9575\n",
      "300 3000 0.003 0.9675\n",
      "300 3000 0.01 0.9425\n",
      "300 10000 1e-05 0.955\n",
      "300 10000 3e-05 0.955\n",
      "300 10000 0.0001 0.9575\n",
      "300 10000 0.0003 0.96\n",
      "300 10000 0.001 0.9625\n",
      "300 10000 0.003 0.9575\n",
      "300 10000 0.01 0.96\n",
      "Best sigma, C, lr are (10.000000, 100.000000, 0.003000) with an accuracy on the validation set of 0.975000.\n",
      "iteration 0 / 30000: loss 100.000000\n",
      "iteration 100 / 30000: loss 1803.144194\n",
      "iteration 200 / 30000: loss 354.349369\n",
      "iteration 300 / 30000: loss 195.253064\n",
      "iteration 400 / 30000: loss 134.352167\n",
      "iteration 500 / 30000: loss 103.315236\n",
      "iteration 600 / 30000: loss 81.587186\n",
      "iteration 700 / 30000: loss 2413.023863\n",
      "iteration 800 / 30000: loss 131.519543\n",
      "iteration 900 / 30000: loss 102.325293\n",
      "iteration 1000 / 30000: loss 82.704266\n",
      "iteration 1100 / 30000: loss 68.112619\n",
      "iteration 1200 / 30000: loss 2399.514117\n",
      "iteration 1300 / 30000: loss 94.368639\n",
      "iteration 1400 / 30000: loss 73.855981\n",
      "iteration 1500 / 30000: loss 62.719516\n",
      "iteration 1600 / 30000: loss 49.744645\n",
      "iteration 1700 / 30000: loss 43.457664\n",
      "iteration 1800 / 30000: loss 88.594371\n",
      "iteration 1900 / 30000: loss 60.151679\n",
      "iteration 2000 / 30000: loss 47.524250\n",
      "iteration 2100 / 30000: loss 41.019953\n",
      "iteration 2200 / 30000: loss 42.907739\n",
      "iteration 2300 / 30000: loss 61.459945\n",
      "iteration 2400 / 30000: loss 45.405634\n",
      "iteration 2500 / 30000: loss 38.286987\n",
      "iteration 2600 / 30000: loss 34.714806\n",
      "iteration 2700 / 30000: loss 31.733440\n",
      "iteration 2800 / 30000: loss 29.717977\n",
      "iteration 2900 / 30000: loss 28.093690\n",
      "iteration 3000 / 30000: loss 28.466889\n",
      "iteration 3100 / 30000: loss 24.968914\n",
      "iteration 3200 / 30000: loss 23.913927\n",
      "iteration 3300 / 30000: loss 23.073773\n",
      "iteration 3400 / 30000: loss 22.427203\n",
      "iteration 3500 / 30000: loss 21.609298\n",
      "iteration 3600 / 30000: loss 20.920284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3700 / 30000: loss 20.185126\n",
      "iteration 3800 / 30000: loss 19.621561\n",
      "iteration 3900 / 30000: loss 18.908397\n",
      "iteration 4000 / 30000: loss 18.564405\n",
      "iteration 4100 / 30000: loss 17.730183\n",
      "iteration 4200 / 30000: loss 17.160617\n",
      "iteration 4300 / 30000: loss 16.566787\n",
      "iteration 4400 / 30000: loss 16.047913\n",
      "iteration 4500 / 30000: loss 15.492350\n",
      "iteration 4600 / 30000: loss 15.068716\n",
      "iteration 4700 / 30000: loss 14.590547\n",
      "iteration 4800 / 30000: loss 14.364386\n",
      "iteration 4900 / 30000: loss 13.774913\n",
      "iteration 5000 / 30000: loss 13.527979\n",
      "iteration 5100 / 30000: loss 13.026667\n",
      "iteration 5200 / 30000: loss 12.967604\n",
      "iteration 5300 / 30000: loss 12.350128\n",
      "iteration 5400 / 30000: loss 12.009084\n",
      "iteration 5500 / 30000: loss 3868.568910\n",
      "iteration 5600 / 30000: loss 57.705348\n",
      "iteration 5700 / 30000: loss 35.303553\n",
      "iteration 5800 / 30000: loss 23.705368\n",
      "iteration 5900 / 30000: loss 19.159569\n",
      "iteration 6000 / 30000: loss 16.778969\n",
      "iteration 6100 / 30000: loss 15.697044\n",
      "iteration 6200 / 30000: loss 14.738392\n",
      "iteration 6300 / 30000: loss 13.889466\n",
      "iteration 6400 / 30000: loss 13.778506\n",
      "iteration 6500 / 30000: loss 12.922145\n",
      "iteration 6600 / 30000: loss 12.457929\n",
      "iteration 6700 / 30000: loss 12.052647\n",
      "iteration 6800 / 30000: loss 11.704044\n",
      "iteration 6900 / 30000: loss 11.352148\n",
      "iteration 7000 / 30000: loss 15.425100\n",
      "iteration 7100 / 30000: loss 10.796437\n",
      "iteration 7200 / 30000: loss 10.520975\n",
      "iteration 7300 / 30000: loss 10.156449\n",
      "iteration 7400 / 30000: loss 9.872006\n",
      "iteration 7500 / 30000: loss 9.828053\n",
      "iteration 7600 / 30000: loss 9.650201\n",
      "iteration 7700 / 30000: loss 9.330306\n",
      "iteration 7800 / 30000: loss 8.912576\n",
      "iteration 7900 / 30000: loss 12.324434\n",
      "iteration 8000 / 30000: loss 8.502871\n",
      "iteration 8100 / 30000: loss 8.231551\n",
      "iteration 8200 / 30000: loss 8.060561\n",
      "iteration 8300 / 30000: loss 7.783325\n",
      "iteration 8400 / 30000: loss 7.678666\n",
      "iteration 8500 / 30000: loss 7.362839\n",
      "iteration 8600 / 30000: loss 7.154980\n",
      "iteration 8700 / 30000: loss 27.520409\n",
      "iteration 8800 / 30000: loss 50.050773\n",
      "iteration 8900 / 30000: loss 26.645646\n",
      "iteration 9000 / 30000: loss 17.171306\n",
      "iteration 9100 / 30000: loss 13.309094\n",
      "iteration 9200 / 30000: loss 11.330361\n",
      "iteration 9300 / 30000: loss 10.164980\n",
      "iteration 9400 / 30000: loss 9.219495\n",
      "iteration 9500 / 30000: loss 8.809859\n",
      "iteration 9600 / 30000: loss 11.809968\n",
      "iteration 9700 / 30000: loss 8.195273\n",
      "iteration 9800 / 30000: loss 7.937894\n",
      "iteration 9900 / 30000: loss 7.654279\n",
      "iteration 10000 / 30000: loss 7.595780\n",
      "iteration 10100 / 30000: loss 7.176508\n",
      "iteration 10200 / 30000: loss 6.967045\n",
      "iteration 10300 / 30000: loss 6.741575\n",
      "iteration 10400 / 30000: loss 6.906536\n",
      "iteration 10500 / 30000: loss 6.394493\n",
      "iteration 10600 / 30000: loss 6.346343\n",
      "iteration 10700 / 30000: loss 6.529314\n",
      "iteration 10800 / 30000: loss 6.039067\n",
      "iteration 10900 / 30000: loss 6.025347\n",
      "iteration 11000 / 30000: loss 5.768305\n",
      "iteration 11100 / 30000: loss 9.799181\n",
      "iteration 11200 / 30000: loss 5.612146\n",
      "iteration 11300 / 30000: loss 5.273436\n",
      "iteration 11400 / 30000: loss 5.172821\n",
      "iteration 11500 / 30000: loss 5.223782\n",
      "iteration 11600 / 30000: loss 4.960782\n",
      "iteration 11700 / 30000: loss 5.184039\n",
      "iteration 11800 / 30000: loss 4.793425\n",
      "iteration 11900 / 30000: loss 4.781931\n",
      "iteration 12000 / 30000: loss 4.629902\n",
      "iteration 12100 / 30000: loss 4.629207\n",
      "iteration 12200 / 30000: loss 4.431973\n",
      "iteration 12300 / 30000: loss 4.433872\n",
      "iteration 12400 / 30000: loss 4.324573\n",
      "iteration 12500 / 30000: loss 43.080469\n",
      "iteration 12600 / 30000: loss 20.436761\n",
      "iteration 12700 / 30000: loss 12.188332\n",
      "iteration 12800 / 30000: loss 9.017580\n",
      "iteration 12900 / 30000: loss 7.448186\n",
      "iteration 13000 / 30000: loss 6.403029\n",
      "iteration 13100 / 30000: loss 5.796713\n",
      "iteration 13200 / 30000: loss 5.637185\n",
      "iteration 13300 / 30000: loss 5.323745\n",
      "iteration 13400 / 30000: loss 5.057326\n",
      "iteration 13500 / 30000: loss 5.044520\n",
      "iteration 13600 / 30000: loss 4.842029\n",
      "iteration 13700 / 30000: loss 4.634710\n",
      "iteration 13800 / 30000: loss 4.394413\n",
      "iteration 13900 / 30000: loss 4.276681\n",
      "iteration 14000 / 30000: loss 4.166816\n",
      "iteration 14100 / 30000: loss 4.224004\n",
      "iteration 14200 / 30000: loss 3.944089\n",
      "iteration 14300 / 30000: loss 4.101346\n",
      "iteration 14400 / 30000: loss 7.335203\n",
      "iteration 14500 / 30000: loss 4.173299\n",
      "iteration 14600 / 30000: loss 3.693589\n",
      "iteration 14700 / 30000: loss 3.802025\n",
      "iteration 14800 / 30000: loss 3.537877\n",
      "iteration 14900 / 30000: loss 3.359113\n",
      "iteration 15000 / 30000: loss 3.472339\n",
      "iteration 15100 / 30000: loss 3.228831\n",
      "iteration 15200 / 30000: loss 3.775613\n",
      "iteration 15300 / 30000: loss 10.076408\n",
      "iteration 15400 / 30000: loss 3.444012\n",
      "iteration 15500 / 30000: loss 3.283514\n",
      "iteration 15600 / 30000: loss 2.965415\n",
      "iteration 15700 / 30000: loss 3.025538\n",
      "iteration 15800 / 30000: loss 3.033378\n",
      "iteration 15900 / 30000: loss 2.938664\n",
      "iteration 16000 / 30000: loss 3.498741\n",
      "iteration 16100 / 30000: loss 2.903881\n",
      "iteration 16200 / 30000: loss 2.788326\n",
      "iteration 16300 / 30000: loss 2.538479\n",
      "iteration 16400 / 30000: loss 2.803910\n",
      "iteration 16500 / 30000: loss 2.764187\n",
      "iteration 16600 / 30000: loss 2.718265\n",
      "iteration 16700 / 30000: loss 4.808752\n",
      "iteration 16800 / 30000: loss 3.783829\n",
      "iteration 16900 / 30000: loss 2.552006\n",
      "iteration 17000 / 30000: loss 2.572676\n",
      "iteration 17100 / 30000: loss 2.443362\n",
      "iteration 17200 / 30000: loss 2.416717\n",
      "iteration 17300 / 30000: loss 2.589669\n",
      "iteration 17400 / 30000: loss 2.455908\n",
      "iteration 17500 / 30000: loss 2.574896\n",
      "iteration 17600 / 30000: loss 3.557404\n",
      "iteration 17700 / 30000: loss 2.323040\n",
      "iteration 17800 / 30000: loss 3.284512\n",
      "iteration 17900 / 30000: loss 2.185076\n",
      "iteration 18000 / 30000: loss 3.094871\n",
      "iteration 18100 / 30000: loss 3.134090\n",
      "iteration 18200 / 30000: loss 3.237146\n",
      "iteration 18300 / 30000: loss 1.866467\n",
      "iteration 18400 / 30000: loss 1.803015\n",
      "iteration 18500 / 30000: loss 1.730282\n",
      "iteration 18600 / 30000: loss 1.819244\n",
      "iteration 18700 / 30000: loss 1.854565\n",
      "iteration 18800 / 30000: loss 1.961899\n",
      "iteration 18900 / 30000: loss 55.946049\n",
      "iteration 19000 / 30000: loss 17.710407\n",
      "iteration 19100 / 30000: loss 8.979388\n",
      "iteration 19200 / 30000: loss 5.844441\n",
      "iteration 19300 / 30000: loss 4.394617\n",
      "iteration 19400 / 30000: loss 3.620919\n",
      "iteration 19500 / 30000: loss 3.129934\n",
      "iteration 19600 / 30000: loss 2.952776\n",
      "iteration 19700 / 30000: loss 2.835689\n",
      "iteration 19800 / 30000: loss 2.668386\n",
      "iteration 19900 / 30000: loss 2.646202\n",
      "iteration 20000 / 30000: loss 2.482274\n",
      "iteration 20100 / 30000: loss 2.447614\n",
      "iteration 20200 / 30000: loss 2.356497\n",
      "iteration 20300 / 30000: loss 2.253818\n",
      "iteration 20400 / 30000: loss 2.166620\n",
      "iteration 20500 / 30000: loss 2.114296\n",
      "iteration 20600 / 30000: loss 2.010033\n",
      "iteration 20700 / 30000: loss 2.224616\n",
      "iteration 20800 / 30000: loss 1.948777\n",
      "iteration 20900 / 30000: loss 1.920760\n",
      "iteration 21000 / 30000: loss 1.903906\n",
      "iteration 21100 / 30000: loss 1.899884\n",
      "iteration 21200 / 30000: loss 2.038089\n",
      "iteration 21300 / 30000: loss 1.790327\n",
      "iteration 21400 / 30000: loss 1.821836\n",
      "iteration 21500 / 30000: loss 1.815116\n",
      "iteration 21600 / 30000: loss 1.625881\n",
      "iteration 21700 / 30000: loss 3.140757\n",
      "iteration 21800 / 30000: loss 3.334011\n",
      "iteration 21900 / 30000: loss 1.678318\n",
      "iteration 22000 / 30000: loss 1.640338\n",
      "iteration 22100 / 30000: loss 1.672885\n",
      "iteration 22200 / 30000: loss 1.493273\n",
      "iteration 22300 / 30000: loss 1.852826\n",
      "iteration 22400 / 30000: loss 1.685568\n",
      "iteration 22500 / 30000: loss 3.321567\n",
      "iteration 22600 / 30000: loss 3.039668\n",
      "iteration 22700 / 30000: loss 1.463585\n",
      "iteration 22800 / 30000: loss 1.403233\n",
      "iteration 22900 / 30000: loss 1.537548\n",
      "iteration 23000 / 30000: loss 1.509422\n",
      "iteration 23100 / 30000: loss 5.651296\n",
      "iteration 23200 / 30000: loss 3.938880\n",
      "iteration 23300 / 30000: loss 2.427126\n",
      "iteration 23400 / 30000: loss 1.347671\n",
      "iteration 23500 / 30000: loss 1.304323\n",
      "iteration 23600 / 30000: loss 1.247436\n",
      "iteration 23700 / 30000: loss 1.355658\n",
      "iteration 23800 / 30000: loss 1.435630\n",
      "iteration 23900 / 30000: loss 1.167303\n",
      "iteration 24000 / 30000: loss 1.403000\n",
      "iteration 24100 / 30000: loss 3.005775\n",
      "iteration 24200 / 30000: loss 3.067979\n",
      "iteration 24300 / 30000: loss 1.198034\n",
      "iteration 24400 / 30000: loss 1.192904\n",
      "iteration 24500 / 30000: loss 1.321449\n",
      "iteration 24600 / 30000: loss 1.211531\n",
      "iteration 24700 / 30000: loss 1.052600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 24800 / 30000: loss 1.073457\n",
      "iteration 24900 / 30000: loss 1.049663\n",
      "iteration 25000 / 30000: loss 1.004457\n",
      "iteration 25100 / 30000: loss 3.410862\n",
      "iteration 25200 / 30000: loss 1.114961\n",
      "iteration 25300 / 30000: loss 1.031377\n",
      "iteration 25400 / 30000: loss 0.982316\n",
      "iteration 25500 / 30000: loss 0.960435\n",
      "iteration 25600 / 30000: loss 0.932242\n",
      "iteration 25700 / 30000: loss 2.875808\n",
      "iteration 25800 / 30000: loss 2.314165\n",
      "iteration 25900 / 30000: loss 0.956215\n",
      "iteration 26000 / 30000: loss 0.952815\n",
      "iteration 26100 / 30000: loss 0.888755\n",
      "iteration 26200 / 30000: loss 0.851491\n",
      "iteration 26300 / 30000: loss 0.858553\n",
      "iteration 26400 / 30000: loss 1.209110\n",
      "iteration 26500 / 30000: loss 1.195036\n",
      "iteration 26600 / 30000: loss 0.795747\n",
      "iteration 26700 / 30000: loss 0.790423\n",
      "iteration 26800 / 30000: loss 0.787614\n",
      "iteration 26900 / 30000: loss 1.173664\n",
      "iteration 27000 / 30000: loss 0.800775\n",
      "iteration 27100 / 30000: loss 0.796958\n",
      "iteration 27200 / 30000: loss 0.777370\n",
      "iteration 27300 / 30000: loss 0.771565\n",
      "iteration 27400 / 30000: loss 0.765309\n",
      "iteration 27500 / 30000: loss 3.706199\n",
      "iteration 27600 / 30000: loss 0.787010\n",
      "iteration 27700 / 30000: loss 0.798775\n",
      "iteration 27800 / 30000: loss 1.190843\n",
      "iteration 27900 / 30000: loss 0.741723\n",
      "iteration 28000 / 30000: loss 0.739133\n",
      "iteration 28100 / 30000: loss 0.750096\n",
      "iteration 28200 / 30000: loss 0.731679\n",
      "iteration 28300 / 30000: loss 3.564510\n",
      "iteration 28400 / 30000: loss 3.195352\n",
      "iteration 28500 / 30000: loss 0.758675\n",
      "iteration 28600 / 30000: loss 0.753400\n",
      "iteration 28700 / 30000: loss 0.763265\n",
      "iteration 28800 / 30000: loss 0.755626\n",
      "iteration 28900 / 30000: loss 0.718683\n",
      "iteration 29000 / 30000: loss 0.714935\n",
      "iteration 29100 / 30000: loss 0.714640\n",
      "iteration 29200 / 30000: loss 1.113168\n",
      "iteration 29300 / 30000: loss 0.946180\n",
      "iteration 29400 / 30000: loss 0.795641\n",
      "iteration 29500 / 30000: loss 0.902577\n",
      "iteration 29600 / 30000: loss 0.894976\n",
      "iteration 29700 / 30000: loss 0.718274\n",
      "iteration 29800 / 30000: loss 0.705302\n",
      "iteration 29900 / 30000: loss 0.696452\n",
      "Accuracy on the training set:  0.99925\n",
      "Accuracy on the testing set:  0.988\n",
      "###### top 15 words ######\n",
      "remot\n",
      "clearli\n",
      "otherwis\n",
      "mondai\n",
      "info\n",
      "wife\n",
      "with\n",
      "dollarac\n",
      "doesn\n",
      "gt\n",
      "human\n",
      "mark\n",
      "militari\n",
      "similar\n",
      "hot\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, metrics\n",
    "import utils\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from linear_classifier import LinearSVM_twoclass\n",
    "\n",
    "# load the SPAM email training dataset\n",
    "\n",
    "X,y = utils.load_mat('data/spamTrain.mat')\n",
    "yy = np.ones(y.shape)\n",
    "yy[y==0] = -1\n",
    "\n",
    "# load the SPAM email test dataset\n",
    "\n",
    "test_data = scipy.io.loadmat('data/spamTest.mat')\n",
    "X_test = test_data['Xtest']\n",
    "y_test = test_data['ytest'].flatten()\n",
    "\n",
    "##################################################################################\n",
    "#  YOUR CODE HERE for training the best performing SVM for the data above.       #\n",
    "#  what should C be? What should num_iters be? Should X be scaled?               #\n",
    "#  should X be kernelized? What should the learning rate be? What should the     #\n",
    "#  number of iterations be?                                                      #\n",
    "##################################################################################\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "svm = LinearSVM_twoclass()\n",
    "svm.theta = np.zeros((X.shape[1],))\n",
    "\n",
    "X_train = X[:3600]\n",
    "yy_train = yy[:3600]\n",
    "X_val = X[3600:]\n",
    "yy_val = yy[3600:]\n",
    "\n",
    "sigma_vals = [0.3,1,3,10,30,100,300]\n",
    "Cvals = [10,30,100,300,1000,3000,10000]\n",
    "learning_rates = [1e-5,3e-5,1e-4,3e-4,1e-3,3e-3,1e-2]\n",
    "\n",
    "bestAcc = 0.0\n",
    "bests = (0, 0, 0)\n",
    "\n",
    "for s in sigma_vals:\n",
    "#    KKtrain = X_train\n",
    "#    KKval = X_val\n",
    "    gamma = 1.0 / (2 * s * s)\n",
    "    Ktrain = rbf_kernel(X_train, X_train, gamma)\n",
    "    scaler = preprocessing.StandardScaler().fit(Ktrain)\n",
    "    scaleKtrain = scaler.transform(Ktrain)\n",
    "    KKtrain = np.vstack([np.ones((scaleKtrain.shape[0],)), scaleKtrain.T]).T\n",
    "    Kval = rbf_kernel(X_val, X_train, gamma)\n",
    "    scaleKval = scaler.transform(Kval)\n",
    "    KKval = np.vstack([np.ones((scaleKval.shape[0],)), scaleKval.T]).T\n",
    "\n",
    "    svm.theta = np.zeros((KKtrain.shape[1],))\n",
    "    for c in Cvals:\n",
    "        for lr in learning_rates:\n",
    "            svm.train(KKtrain, yy_train, learning_rate=lr, reg=c, num_iters=200, batch_size=400)\n",
    "            yy_val_pre = svm.predict(KKval)\n",
    "            curAcc = np.mean((yy_val_pre == yy_val) * 1.0)\n",
    "            print s, c, lr, curAcc\n",
    "            if curAcc > bestAcc:\n",
    "                bestAcc = curAcc\n",
    "                bests = (s, c, lr)\n",
    "                best_svm = svm\n",
    "            \n",
    "print \"Best sigma, C, lr are (%f, %f, %f) with an accuracy on the validation set of %f.\" %(bests[0], bests[1], bests[2], bestAcc)\n",
    "\n",
    "##################################################################################\n",
    "# YOUR CODE HERE for testing your best model's performance                       #\n",
    "# what is the accuracy of your best model on the test set? On the training set?  #\n",
    "##################################################################################\n",
    "\n",
    "s = bests[0]\n",
    "c = bests[1]\n",
    "lr = bests[2]\n",
    "#KK = X\n",
    "gamma = 1.0 / (2 * s * s)\n",
    "K = rbf_kernel(X, X, gamma)\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)), scaleK.T]).T\n",
    "\n",
    "best_svm.theta = np.zeros((KK.shape[1],))\n",
    "best_svm.train(KK, yy, learning_rate=lr, reg=c, num_iters=30000, batch_size=KK.shape[0], verbose=True)\n",
    "yy_pre = best_svm.predict(KK)\n",
    "acc_train = np.mean((yy_pre == yy) * 1.0)\n",
    "print \"Accuracy on the training set: \", acc_train\n",
    "\n",
    "yy_test = np.ones(y_test.shape)\n",
    "yy_test[y_test==0] = -1\n",
    "#KKtest = X_test\n",
    "Ktest = rbf_kernel(X_test, X, gamma)\n",
    "scaleKtest = scaler.transform(Ktest)\n",
    "KKtest = np.vstack([np.ones((scaleKtest.shape[0],)), scaleKtest.T]).T\n",
    "\n",
    "yy_test_pre = best_svm.predict(KKtest)\n",
    "acc_test = np.mean((yy_test_pre == yy_test) * 1.0)\n",
    "print \"Accuracy on the testing set: \", acc_test\n",
    "\n",
    "##################################################################################\n",
    "# ANALYSIS OF MODEL: Print the top 15 words that are predictive of spam and for  #\n",
    "# ham. Hint: use the coefficient values of the learned model                     #\n",
    "##################################################################################\n",
    "words, inv_words = utils.get_vocab_dict()\n",
    "\n",
    "print \"###### top 15 words ######\"\n",
    "t = np.dot(best_svm.theta[1:], X).argsort()[::-1]\n",
    "w15 = t[:15]\n",
    "for w in w15:\n",
    "    print words[w]\n",
    "##################################################################################\n",
    "#                    END OF YOUR CODE                                            #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
