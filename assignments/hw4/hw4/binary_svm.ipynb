{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "In the first part of this exercise, you will  build  support vector machines\n",
    "(SVMs) for solving  binary classification problems. You will experiment with your\n",
    "classifier on three example 2D datasets. Experimenting with these datasets\n",
    "will help you gain  intuition into how SVMs work and how to use a Gaussian\n",
    "kernel with SVMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set 1\n",
    "We will begin  with a 2D example dataset which can be separated by a\n",
    "linear boundary.  In\n",
    "this dataset, the positions of the positive examples (green circles) and the\n",
    "negative examples (indicated with red circles) suggest a natural separation indicated\n",
    "by the gap. However, notice that there is an outlier positive example  on\n",
    "the far left at about (0.1, 4.1). As part of this exercise, you will also see how\n",
    "this outlier affects the SVM decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import linear_svm\n",
    "import utils\n",
    "from sklearn import preprocessing, metrics\n",
    "from linear_classifier import LinearSVM_twoclass\n",
    "\n",
    "############################################################################\n",
    "#  Part  0: Loading and Visualizing Data                                   #\n",
    "#  We start the exercise by first loading and visualizing the dataset.     #\n",
    "#  The following code will load the dataset into your environment and plot #\n",
    "#  the data.                                                               #\n",
    "############################################################################\n",
    "\n",
    "# load ex4data1.mat\n",
    "\n",
    "X,y = utils.load_mat('data/ex4data1.mat')\n",
    "\n",
    "utils.plot_twoclass_data(X,y,'x1', 'x2',['neg','pos'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hinge loss function and gradient\n",
    "Now you will implement the hinge loss cost function and its gradient for support vector machines.\n",
    "Complete the **binary\\_svm\\_loss** function in **linear\\_svm.py**  to return the cost and gradient for the hinge loss function.\n",
    " Recall that the hinge loss function is\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{j=0}^{d} {\\theta_j}^2 + \\frac{C}{m} \\sum_{i=1}^{m} max(0, 1 -y^{(i)}h_\\theta(x^{(i)})) $$\n",
    "\n",
    "where $h_{\\theta}(x) = \\theta^ T x$ with $x_0 = 1$.  $C$ is the penalty factor which measures how much misclassifications are penalized. If $y^{(i)}h_\\theta(x^{(i)})) > 1$, then $x^{(i)}$ is correctly classified and the loss associated with that example is zero. If $y^{(i)}h_\\theta(x^{(i)})) < 1$, then $x^{(i)}$ is not within the appropriate margin (positive or negative) and the loss associated with that example is greater than zero. The gradient of the hinge loss\n",
    "function  is a vector of the same length as $\\theta$ where the $j^{th}$ element, $j=0,1,\\ldots,d$ is defined as follows:\n",
    "\n",
    "\\begin{eqnarray*} \\frac{\\partial J(\\theta)}{\\partial \\theta_j}  & = &\n",
    "\\left \\{\n",
    "\\begin{array}{l l}\n",
    "\\frac{1}{m} \\theta_j + \\frac{C}{m} \\sum_{i=1}^{m} -y^{(i)}x_j^{(i)}& \\mbox{ if } y^{(i)}h_\\theta(x^{(i)})) < 1\\\\\n",
    "\\frac{1}{m} \\theta_j & \\mbox{ if } y^{(i)}h_\\theta(x^{(i)})) >= 1\\\\\n",
    "\\end{array} \\right. \n",
    "\\end{eqnarray*}\n",
    "\n",
    "Once you are done, the cell below will call your **binary\\_svm\\_loss** function with a zero vector $\\theta$.\n",
    " You should see that the cost $J$ is 1.0. The gradient of the loss function with respect to an all-zeros $\\theta$ vector is also computed and should be $[-0.12956186 -0.00167647]^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#  Part 1: Hinge loss function and gradient                                #\n",
    "############################################################################\n",
    "\n",
    "C = 1\n",
    "theta = np.zeros((X.shape[1],))\n",
    "\n",
    "yy = np.ones(y.shape)\n",
    "yy[y==0] = -1\n",
    "J,grad = linear_svm.binary_svm_loss(theta,X,yy,C)\n",
    "\n",
    "print \"J = \", J, \" grad = \", grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of varying C\n",
    "In this part of the exercise, you will try using different values of the C\n",
    "parameter with SVMs. Informally, the C parameter is a positive value that\n",
    "controls the penalty for misclassified training examples. A large C parameter\n",
    "tells the SVM to try to classify all the examples correctly. C plays a role\n",
    "similar to $\\frac{1}{\\lambda}$, where $\\lambda$ is the regularization parameter that we were using\n",
    "previously for logistic regression.\n",
    "\n",
    "The SVM training function is in **linear\\_classifier.py** -- this is a gradient descent algorithm that uses your loss and gradient functions. \n",
    "The cell below will train an SVM on the example data set 1 with C = 1. It first scales the data to have zero mean and unit variance, and adds the intercept term to the data matrix.\n",
    "When C = 1, you should find that the SVM puts the decision boundary in\n",
    "the gap between the two datasets and misclassifies the data point on the far\n",
    "left. \n",
    "\n",
    "Your task is to try different values of C on this dataset. Specifically, you\n",
    "should change the value of C in the cell below to C = 100 and run the SVM\n",
    "training again. When C = 100, you should find that the SVM now classifies\n",
    "every single example correctly, but has a decision boundary that does not\n",
    "appear to be a natural fit for the data. Include a plot of the decision boundary for C = 100 in writeup.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Scale the data and set up the SVM training                               #\n",
    "############################################################################\n",
    "\n",
    "# scale the data\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "scaleX = scaler.transform(X)\n",
    "\n",
    "# add an intercept term and convert y values from [0,1] to [-1,1]\n",
    "\n",
    "XX = np.array([(1,x1,x2) for (x1,x2) in scaleX])\n",
    "yy = np.ones(y.shape)\n",
    "yy[y == 0] = -1\n",
    "yy[y == 0] = -1\n",
    "\n",
    "############################################################################\n",
    "#  Part  2: Training linear SVM                                            #\n",
    "#  We train a linear SVM on the data set and the plot the learned          #\n",
    "#  decision boundary                                                       #\n",
    "############################################################################\n",
    "\n",
    "############################################################################\n",
    "# You will change this line below to vary C.                               #\n",
    "############################################################################\n",
    "\n",
    "C = 1\n",
    "\n",
    "svm = LinearSVM_twoclass()\n",
    "svm.theta = np.zeros((XX.shape[1],))\n",
    "svm.train(XX,yy,learning_rate=1e-4,reg=C,num_iters=50000,verbose=True,,batch_size=XX.shape[0])\n",
    "\n",
    "# classify the training data\n",
    "\n",
    "y_pred = svm.predict(XX)\n",
    "\n",
    "print \"Accuracy on training data = \", metrics.accuracy_score(yy,y_pred)\n",
    "\n",
    "# visualize the decision boundary\n",
    "\n",
    "utils.plot_decision_boundary(scaleX,y,svm,'x1','x2',['neg','pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs with Gaussian kernels \n",
    "In this part of the exercise, you will be using SVMs to do non-linear classification.\n",
    "In particular, you will be using SVMs with Gaussian kernels on\n",
    "datasets that are not linearly separable.\n",
    "\n",
    "To find non-linear decision boundaries with the SVM, we need to first implement\n",
    "a Gaussian kernel. You can think of the Gaussian kernel as a similarity\n",
    "function that measures the distance between a pair of examples,\n",
    "$(x^{(i)}, x^{(j)})$. The Gaussian kernel is also parameterized by a bandwidth parameter,\n",
    "$\\sigma$,  which determines how fast the similarity metric decreases (to 0)\n",
    "as the examples are further apart.\n",
    "You should now complete the function **gaussian\\_kernel** in **utils.py** to compute\n",
    "the Gaussian kernel between two examples. The Gaussian kernel\n",
    "function is defined as:\n",
    "\n",
    "$$ k(x^{(i)},x^{(j)}) = exp\\left(- \\frac{{||x^{(i)}-x^{(j)}||}^2}{2\\sigma^2}\\right) $$\n",
    "\n",
    "When you have completed the function, the cell below\n",
    "will test your kernel function on two provided examples and you should expect\n",
    "to see a value of 0.324652."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#  Part  3: Training SVM with a kernel                                     #\n",
    "#  We train an SVM with an RBF kernel on the data set and the plot the     #\n",
    "#  learned decision boundary                                               #\n",
    "############################################################################\n",
    "\n",
    "# test your Gaussian kernel implementation\n",
    "\n",
    "x1 = np.array([1,2,1])\n",
    "x2 = np.array([0,4,-1])\n",
    "sigma = 2\n",
    "\n",
    "print \"Guassian kernel value (should be around 0.324652) = \", utils.gaussian_kernel(x1,x2,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs with Gaussian kernels on Dataset 2\n",
    "The next cell will load and plot dataset 2. From\n",
    "the plot, you can observe that there is no linear decision boundary that\n",
    "separates the positive and negative examples for this dataset. However, by\n",
    "using the Gaussian kernel with the SVM, you will be able to learn a non-linear\n",
    "decision boundary that can perform reasonably well for the dataset.\n",
    "If you have correctly implemented the Gaussian kernel function, the cell below\n",
    "will proceed to train the SVM with the Gaussian kernel on this dataset.\n",
    "\n",
    "The decision boundary found by the SVM with C = 1 and a Gaussian\n",
    "kernel with $\\sigma = 0.01$ will be plotted. The decision boundary is able to separate most of the positive and\n",
    "negative examples correctly and follows the contours of the dataset well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ex4data2.mat\n",
    "\n",
    "X,y = utils.load_mat('data/ex4data2.mat')\n",
    "\n",
    "# visualize the data\n",
    "\n",
    "utils.plot_twoclass_data(X,y,'', '',['neg','pos'])\n",
    "\n",
    "# convert X to kernel form with the kernel function\n",
    "\n",
    "sigma = 0.02\n",
    "\n",
    "# compute the kernel (slow!)\n",
    "\n",
    "K = np.array([utils.gaussian_kernel(x1,x2,sigma) for x1 in X for x2 in X]).reshape(X.shape[0],X.shape[0])\n",
    "\n",
    "# scale the kernelized data matrix\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(K)\n",
    "scaleK = scaler.transform(K)\n",
    "\n",
    "# add the intercept term\n",
    "\n",
    "KK = np.vstack([np.ones((scaleK.shape[0],)),scaleK.T]).T\n",
    "\n",
    "# transform y from [0,1] to [-1,1]\n",
    "\n",
    "yy = np.ones(y.shape)\n",
    "yy[y == 0] = -1\n",
    "\n",
    "# set up the SVM and learn the parameters\n",
    "\n",
    "svm = LinearSVM_twoclass()\n",
    "svm.theta = np.zeros((KK.shape[1],))\n",
    "C = 1\n",
    "svm.train(KK,yy,learning_rate=1e-4,reg=C,num_iters=20000,verbose=True,batch_size=KK.shape[0])\n",
    "\n",
    "# visualize the boundary\n",
    "\n",
    "utils.plot_decision_kernel_boundary(X,y,scaler,sigma,svm,'','',['neg','pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting hyperparameters for SVMs with Gaussian kernels\n",
    "In this part of the exercise, you will gain more practical skills on how to use\n",
    "a SVM with a Gaussian kernel. The next cell will load and display\n",
    "a third dataset. \n",
    "In the provided dataset, **ex4data3.mat**, you are given the variables **X**,\n",
    "**y**, **Xval**, **yval**.  You will be using the SVM with the Gaussian\n",
    "kernel with this dataset. Your task is to use the  validation set **Xval**, **yval** to determine the\n",
    "best C and $\\sigma$ parameter to use. You should write any additional code necessary\n",
    "to help you search over the parameters C and $\\sigma$. For both C and $\\sigma$, we\n",
    "suggest trying values in multiplicative steps (e.g., 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30).\n",
    "Note that you should try all possible pairs of values for C and $\\sigma$ (e.g., C = 0.3\n",
    "and $\\sigma$ = 0.1). For example, if you try each of the 8 values listed above for C\n",
    "and for $\\sigma$, you would end up training and evaluating (on the  validation\n",
    "set) a total of $8^2 = 64$ different models.\n",
    "\n",
    "When  selecting  the\n",
    "best C and $\\sigma$ parameter to use, you train on {\\tt X,y} with a given C and $\\sigma$, and then  evaluate the error of the model on the \n",
    "validation set. Recall that for classification, the error is defined as the\n",
    "fraction of the  validation examples that were classified incorrectly.\n",
    "You can use the **predict** method of the SVM classifier to generate the predictions for the\n",
    " validation set.\n",
    " \n",
    " After you have determined the best C and $\\sigma$ parameters to use, you\n",
    "should replace the assignments to **best\\_C** and **best\\_sigma** in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#  Part  4: Training SVM with a kernel                                     #\n",
    "#  We train an SVM with an RBF kernel on the data set and the plot the     #\n",
    "#  learned decision boundary                                               #\n",
    "############################################################################\n",
    "\n",
    "# load ex4data3.mat\n",
    "\n",
    "X,y,Xval,yval = utils.loadval_mat('data/ex4data3.mat')\n",
    "\n",
    "# transform y and yval from [0,1] to [-1,1]\n",
    "\n",
    "yy = np.ones(y.shape)\n",
    "yy[y == 0] = -1\n",
    "\n",
    "yyval = np.ones(yval.shape)\n",
    "yyval[yval == 0] = -1\n",
    "\n",
    "# visualize the data\n",
    "\n",
    "utils.plot_twoclass_data(X,y,'x1', 'x2',['neg','pos'])\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# TODO                                                                     #\n",
    "# select hyperparameters C and sigma for this dataset using                #\n",
    "# Xval and yval                                                            #\n",
    "############################################################################\n",
    "Cvals = [0.01,0.03,0.1,0.3,1,3,10,30]\n",
    "sigma_vals = [0.01,0.03,0.1,0.3,1,3,10,30]\n",
    "\n",
    "# TODO       \n",
    "# select hyperparameters here; loopover all Cvals and sigma_vals. \n",
    "# About 8-10 lines of code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#       END OF YOUR CODE                                                   #\n",
    "############################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: make sure you put in the best_C and best_sigma from the analysis above!\n",
    "\n",
    "\n",
    "best_C = None\n",
    "best_sigma = None\n",
    "\n",
    "# train an SVM on (X,y) with best_C and best_sigma\n",
    "best_svm = LinearSVM_twoclass()\n",
    "\n",
    "############################################################################\n",
    "# TODO: construct the Gram matrix of the data with best_sigma, scale it, add the column of ones\n",
    "# Then use svm_train to train best_svm with the best_C parameter. Use 20,000 iterations and\n",
    "# a learning rate of 1e-4. Use batch_size of the entire training data set.\n",
    "# About 5-6 lines of code expected here.\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#       END OF YOUR CODE                                                   #\n",
    "############################################################################\n",
    "\n",
    "# visualize the boundary (uncomment this line after you learn the best svm)\n",
    "\n",
    "utils.plot_decision_kernel_boundary(X,y,scaler,best_sigma,best_svm,'','',['neg','pos'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
