{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using regularized logistic regression to classify email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Penalty experiments -----------\n",
      "best_lambda =  0.1\n",
      "Coefficients =  [-4.86311363] [[ -2.74146423e-02  -2.25297597e-01   1.21840933e-01   2.29362879e+00\n",
      "    2.70425715e-01   2.32851163e-01   9.28595395e-01   2.95200236e-01\n",
      "    1.62205936e-01   6.78260362e-02  -8.32604386e-02  -1.60373354e-01\n",
      "   -4.72247682e-02   1.07677111e-02   1.87903360e-01   8.19771812e-01\n",
      "    5.09528973e-01   3.98711504e-02   2.67729695e-01   3.47047564e-01\n",
      "    2.60498923e-01   3.64605215e-01   7.25019578e-01   1.96728249e-01\n",
      "   -3.15395701e+00  -4.03133789e-01  -1.25451044e+01  -6.16580960e-02\n",
      "   -1.56114609e+00  -5.51429801e-02  -3.00815864e-02   4.07263543e-01\n",
      "   -3.68156446e-01  -1.43611787e+00  -5.87180606e-01   4.44294891e-01\n",
      "    4.23159462e-02  -1.56897094e-01  -4.55330838e-01  -1.02250289e-01\n",
      "   -3.54273295e+00  -1.72944487e+00  -4.37529300e-01  -1.05999941e+00\n",
      "   -9.18599328e-01  -1.75490328e+00  -1.67475856e-01  -9.56875266e-01\n",
      "   -3.65653149e-01  -1.36535510e-01  -6.58692488e-02   2.06714030e-01\n",
      "    1.70694385e+00   1.21460313e+00  -3.35269880e-01   1.56141411e+00\n",
      "    3.68775701e-01]]\n",
      "Accuracy on set aside test set for  std  =  0.9296875\n",
      "best_lambda =  0.6\n",
      "Coefficients =  [-4.60944667] [[-0.45145887 -0.28466502 -0.06327819  0.68295804  1.21053197  0.91504987\n",
      "   2.83046244  1.43677856  0.24145457  0.35775723 -0.38642638 -0.48142751\n",
      "  -0.69586861  0.37457025  0.64885478  1.5395627   1.38118339  0.07197747\n",
      "   0.37642217  0.63501926  0.52274838  0.38563703  2.00138627  1.50817467\n",
      "  -3.14060836 -0.66617179 -4.90648468 -0.03260466 -1.28886313 -0.15745822\n",
      "  -0.6389963  -0.30229058 -1.00990215 -0.42568565 -1.08721623  1.28432651\n",
      "  -0.90558978 -0.35285879 -1.12971405 -0.62589095 -1.40337046 -2.44123337\n",
      "  -1.55653325 -1.9477813  -1.13113514 -2.79991122 -0.7512231  -2.11601915\n",
      "  -1.68510766 -0.66773402 -0.69125555  2.06913245  4.21977733  0.7630898\n",
      "   0.70345803  0.17008574  0.43018819]]\n",
      "Accuracy on set aside test set for  logt  =  0.943359375\n",
      "best_lambda =  1.1\n",
      "Coefficients =  [-1.83742964] [[ -1.91463199e-01  -1.66872958e-01  -3.93802023e-01   2.39462779e-01\n",
      "    9.83292893e-01   1.75311414e-01   2.12183419e+00   7.92547596e-01\n",
      "    1.94566579e-01   3.34388296e-01  -2.90824615e-01  -4.20297341e-01\n",
      "   -9.06380381e-01   2.56299856e-01   5.15189474e-01   1.47014136e+00\n",
      "    8.76696476e-01  -8.32760956e-02   2.41264180e-01   5.01801273e-01\n",
      "    7.37046896e-01   1.15518007e+00   9.11195183e-01   1.36902984e+00\n",
      "   -2.35248856e+00  -4.17190306e-01  -3.79772643e+00   6.88337611e-01\n",
      "   -6.07237597e-01  -1.61622832e-01  -9.24671804e-01  -6.04558748e-01\n",
      "   -6.91161481e-01  -3.85638231e-02  -6.71440136e-01   3.52732370e-01\n",
      "   -1.05408408e+00   5.28551480e-01  -7.65306731e-01  -2.46067578e-01\n",
      "   -1.27643951e+00  -1.90613122e+00  -7.90184279e-01  -1.57619158e+00\n",
      "   -7.64312034e-01  -2.22366816e+00  -8.34144234e-02  -1.39371572e+00\n",
      "   -3.06993897e-01   2.00231957e-01  -1.70968577e-01   1.20762876e+00\n",
      "    1.45771409e+00   3.79908693e-02   5.31813313e-04   5.31813313e-04\n",
      "    5.31813313e-04]]\n",
      "Accuracy on set aside test set for  bin  =  0.927734375\n",
      "L1 Penalty experiments -----------\n",
      "best_lambda =  4.6\n",
      "Coefficients =  [-1.59528405] [[-0.01066616 -0.15872646  0.12269348  0.20794984  0.24914094  0.1770254\n",
      "   0.91014923  0.28982802  0.13971357  0.04855675 -0.02304995 -0.13981996\n",
      "  -0.00706598  0.0091887   0.15331756  0.75699007  0.45992978  0.07035737\n",
      "   0.25407585  0.19633242  0.24326464  0.3465077   0.72631226  0.23410927\n",
      "  -2.34056308 -0.35781535 -3.18097986 -0.01105547 -0.37047654  0.          0.\n",
      "   0.         -0.32746025  0.         -0.06139001  0.24253727  0.\n",
      "  -0.11600145 -0.31161077 -0.04369546 -0.23960745 -0.7957659  -0.19073827\n",
      "  -0.56433853 -0.73384605 -1.18124337 -0.08551656 -0.51372379 -0.25639933\n",
      "  -0.13349696 -0.05666312  0.21824111  1.647554    0.22120305  0.\n",
      "   0.64797357  0.33268323]]\n",
      "Accuracy on set aside test set for  std  =  0.921875\n",
      "best_lambda =  1.6\n",
      "Coefficients =  [-4.46325713] [[-0.34343126 -0.09561861  0.          0.12935319  1.18459629  0.69052579\n",
      "   2.9132594   1.37677777  0.          0.29514542  0.         -0.48050956\n",
      "  -0.32797159  0.1073368   0.          1.49525228  1.34902212  0.\n",
      "   0.35495488  0.19675206  0.49438339  0.34756986  1.78473926  1.32956644\n",
      "  -3.49784925 -0.2696488  -7.49408231  0.         -0.41744167  0.          0.\n",
      "   0.         -0.79141606  0.         -0.23866668  0.87194694 -0.77437753\n",
      "   0.         -0.88292016  0.         -0.30404293 -2.35514287 -0.69461756\n",
      "  -1.6613016  -1.13789558 -2.98259264  0.         -1.90116358 -1.24510114\n",
      "  -0.303593    0.          2.01475269  5.36669125  0.          0.63671312\n",
      "   0.20187949  0.3881424 ]]\n",
      "Accuracy on set aside test set for  logt  =  0.944010416667\n",
      "best_lambda =  3.6\n",
      "Coefficients =  [-0.6073033] [[ 0.          0.         -0.19319604  0.          0.86546434  0.\n",
      "   2.02981932  0.63291657  0.02637316  0.21313141  0.         -0.42109053\n",
      "  -0.68156116  0.          0.          1.31585884  0.76539192  0.\n",
      "   0.10412766  0.12293073  0.63680137  0.73007646  0.6219361   1.18349939\n",
      "  -2.42457594 -0.12536816 -3.73175969  0.          0.          0.          0.\n",
      "   0.         -0.28795755  0.         -0.21895667  0.         -1.01577875\n",
      "   0.         -0.40494512  0.         -0.11589481 -1.69459205 -0.03936507\n",
      "  -1.10978283 -0.68778616 -2.21928382  0.         -1.02579743 -0.12514299\n",
      "   0.07431402  0.          1.15123975  1.49962828  0.         -0.55358153\n",
      "  -0.08874693 -0.41567272]]\n",
      "Accuracy on set aside test set for  bin  =  0.92578125\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import utils\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "# No modifications in this script\n",
    "# complete the functions in util.py; then run the script\n",
    "\n",
    "# load the spam data in\n",
    "\n",
    "Xtrain,Xtest,ytrain,ytest = utils.load_spam_data()\n",
    "\n",
    "# Preprocess the data \n",
    "\n",
    "Xtrain_std,mu,sigma = utils.std_features(Xtrain)\n",
    "Xtrain_logt = utils.log_features(Xtrain)\n",
    "Xtrain_bin = utils.bin_features(Xtrain)\n",
    "\n",
    "Xtest_std = (Xtest - mu)/sigma\n",
    "Xtest_logt = utils.log_features(Xtest)\n",
    "Xtest_bin = utils.bin_features(Xtest)\n",
    "\n",
    "# find good lambda by cross validation for these three sets\n",
    "\n",
    "def run_dataset(X,ytrain,Xt,ytest,type,penalty):\n",
    "\n",
    "    best_lambda = utils.select_lambda_crossval(X,ytrain,0.1,5.1,0.5,penalty)\n",
    "    print \"best_lambda = \", best_lambda\n",
    "\n",
    "    # train a classifier on best_lambda and run it\n",
    "    if penalty == \"l2\":\n",
    "        lreg = linear_model.LogisticRegression(penalty=penalty,C=1.0/best_lambda, solver='lbfgs',fit_intercept=True)\n",
    "    else:\n",
    "        lreg = linear_model.LogisticRegression(penalty=penalty,C=1.0/best_lambda, solver='liblinear',fit_intercept=True)\n",
    "    lreg.fit(X,ytrain)\n",
    "    print \"Coefficients = \", lreg.intercept_,lreg.coef_\n",
    "    predy = lreg.predict(Xt)\n",
    "    print \"Accuracy on set aside test set for \", type, \" = \", np.mean(predy==ytest)\n",
    "\n",
    "print \"L2 Penalty experiments -----------\"\n",
    "run_dataset(Xtrain_std,ytrain,Xtest_std,ytest,\"std\",\"l2\")\n",
    "run_dataset(Xtrain_logt,ytrain,Xtest_logt,ytest,\"logt\",\"l2\")\n",
    "run_dataset(Xtrain_bin,ytrain,Xtest_bin,ytest,\"bin\",\"l2\")\n",
    "\n",
    "print \"L1 Penalty experiments -----------\"\n",
    "run_dataset(Xtrain_std,ytrain,Xtest_std,ytest,\"std\",\"l1\")\n",
    "run_dataset(Xtrain_logt,ytrain,Xtest_logt,ytest,\"logt\",\"l1\")\n",
    "run_dataset(Xtrain_bin,ytrain,Xtest_bin,ytest,\"bin\",\"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on the model sparsities with L1 and L2 regularization. \n",
    "#Which class of models will you recommend for this data set and why?\n",
    "\n",
    "L1 regularization will have more sparse coefficients after regularization. And I may recommend the L1 models for this data set because not all the features of the data is crucial for a spam classification problem. But some of the features should have a hight weight when we decide if the emails are spams. Thus, the sparsity of L1 regularization will help us focus on the important features and reduce the impact of insignificant features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
