{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing softmax regression for the CIFAR-10 dataset\n",
    "In this assignment, you will implement a multi-class logistic regression, or softmax regression classifier, and apply it to a version of the CIFAR-10 object recognition dataset. This process involves writing vectorized versions of the softmax loss and gradient functions, choosing learning rates and regularization parameters for the optimization algorithm, and visualizing the learned coefficients of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CIFAR-10 dataset\n",
    "Open up a terminal window and navigate to the **datasets** folder inside the  **hw3** folder. Run the\n",
    "**get\\_datasets.sh**  script. On my Mac, I just type in **./get\\_datasets.sh** at the shell prompt.\n",
    "A new folder called **cifar\\_10\\_batches\\_py** will be created and it will contain $50000$ labeled\n",
    "images for training and $10000$ labeled images for testing. The function further partitions the $50000$ training \n",
    "images into a train set and a validation set for selection of hyperparameters. We have provided a function to\n",
    "read this data in **utils.py**. Each image is a $32 \\times 32$ array of RGB triples. It is preprocessed by\n",
    "subtracting the mean image from all images. We flatten each image into a 1-dimensional array of size\n",
    "3072 (i.e., $32\\times 32 \\times 3$). Then a 1 is appended to the front of that vector to handle \n",
    "the intercept term.  So the training set is a numpy matrix of size $49000\\times 3073$, \n",
    "the validation set is a matrix of size $1000\\times 3073$ and the set-aside test set \n",
    "is of size $10000\\times 3073$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Get the CIFAR-10 data broken up into train, validation and test sets\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = utils.get_CIFAR10_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the loss function for softmax regression (naive version) \n",
    "Softmax regression generalizes logistic regression to classification problems where the class label $y$ can take on more than two possible values. This is useful for such problems as music genre classification and object recognition, where the goal is to distinguish between more than two different music genres or more than two different object categories.  Softmax regression is a supervised learning algorithm, but we will later be using it in conjunction with deep learning and unsupervised feature learning methods.\n",
    "Recall that we are  given a data set \n",
    "$${\\cal D} = \\{(x^{(i)},y^{(i)}) | 1 \\leq i \\leq m; x^{(i)} \\in \\Re^{d+1}; {x_0}^{(i)} = 1, y^{(i)} \\in \\{1,\\ldots, K\\}\\}, K>2 $$\n",
    "Our probabilistic model $h_\\theta(x)$ is defined as\n",
    "\\begin{eqnarray*}\n",
    " h_\\theta(x) & = & \n",
    " \\left [ \\begin{array}{l}\n",
    " P(y = 1|x; \\theta) \\\\\n",
    " P(y = 2| x; \\theta) \\\\\n",
    " \\ldots \\\\\n",
    " P(y=K|x;\\theta)\n",
    " \\end{array} \\right ]\n",
    " \\end{eqnarray*}\n",
    " where \n",
    " $$ P(y=k|x; \\theta) = \\frac{exp({\\theta^{(k)}}^T x)} {\\sum_{j=1}^{K} exp({\\theta^{(j)}}^T x)} $$\n",
    "\n",
    "The parameter  $\\theta$ is a $(d+1)\\times K$ matrix, where each column represents the parameter vector for class $k = 1,\\ldots,K$.\n",
    "$$\\theta = \\left [ \\begin{array}{llll}\n",
    "| & | & \\ldots & | \\\\\n",
    "| & | & \\ldots & | \\\\\n",
    "\\theta^{(1)} & \\theta^{(2)} & \\ldots & \\theta^{(K)} \\\\\n",
    "|&  | & \\ldots & | \\\\\n",
    "| & | & \\ldots  &| \\\\\n",
    "\\end{array}\n",
    "\\right ] $$\n",
    "\n",
    "Numerical stability issues can come up in the computation of $P(y=k|x;\\theta)$. Consider K=3, and $\\theta^T x = [123, 456, 789]$. To compute $P(y=k|x;\\theta)$ from these scores, we need to calculate $exp(123)$, $exp(456)$ and $exp(789)$, and sum them. These are very large numbers. However, we can get the same probabilities by subtracting the maximum ($789$) from  every element in $\\theta^Tx$. Then we have the vector\n",
    "$[-666,-333,0]$, and we can calculate $exp(-666)$, $exp(-333)$ and $exp(0)$, sum them (call the sum $S$) and then calculate $exp(-666)/S$, $exp(-333/S)$ and $exp(0)/S$.\n",
    " \n",
    "The cost function $J(\\theta)$ for softmax regression is derived from the negative log likelihood of the data ${\\cal D}$, assuming that  $P(y|x;\\theta) = h_\\theta(x)$ as defined above.\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{k=1}^{K} I\\{y^{(i)} = k\\} log \\frac{exp({\\theta^{(k)}}^T x^{(i)})}{\\sum_{j=1}^{K} exp({\\theta^{(j)}}^T x^{(i)})}  + \\frac{\\lambda}{2m} \\sum_{j=0}^{d}\\sum_{k=1}^{K} {{\\theta_j}^{(k)}}^2 $$\n",
    "\n",
    "where $I\\{c\\}$ is the indicator function which evaluates to 1 when $c$ is a true statement and to $0$ otherwise. The second term is a regularization term, where $\\lambda$ is the regularization strength. While it is customary to exclude the bias term in L2 regularization, we include it here because it does not make a huge difference in the final result. You can check this for yourself on the CIFAR-10 dataset. You should implement this loss function using **for** loops for the summations in the function **softmax\\_loss\\_naive** in **softmax.py**. Once you have the loss function implemented, the cell below will run your loss function for a randomly initialized $\\theta$ matrix with 49000 training images and labels with $\\lambda$ set to 0. You should expect to see a value of about $-log_e(0.1)$ (Why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softmax import softmax_loss_naive, softmax_loss_vectorized\n",
    "\n",
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "# Generate a random softmax theta matrix and use it to compute the loss.\n",
    "\n",
    "theta = np.random.randn(3073,10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(theta, X_train, y_train, 0.0)\n",
    "\n",
    "# Loss should be something close to 2.38\n",
    "\n",
    "print 'loss: (should be close to 2.38): ', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the gradient of loss function for softmax regression (naive version) \n",
    "The derivative of the loss function $J(\\theta)$ with respect to the $\\theta^{(k)}$ is\n",
    "\n",
    "$$ \\nabla_{\\theta^{(k)}} J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [x^{(i)} (1\\{y^{(i)}=k\\} - P(y^{(i)}=k|x^{(i)};\\theta))] +\\frac{\\lambda}{m} \\theta^{(k)}$$\n",
    "\n",
    "Implement the analytical derivative computation in **softmax\\_loss\\_naive** in **softmax.py**.\n",
    "\n",
    "We  check your implementation of the gradient using the method of finite differences. The functions in ** gradient\\_check.py** compute the numerical gradient of a function $f$ as follows:\n",
    "$$ \\frac{\\partial f(x)}{\\partial x} = \\frac{f(x+h)-f(x-h)}{2h} $$\n",
    "for a very small $h$. The cell below will check your gradient against the numerically approximated gradient -- you should expect to see differences between the two gradients of the order of $10^{-7}$ or less. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Open the file softmax.py and implement the gradient in the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "# Use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient. (within 1e-7)\n",
    "\n",
    "from gradient_check import grad_check_sparse\n",
    "f = lambda th: softmax_loss_naive(th, X_train, y_train, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, theta, grad, 10)\n",
    "\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(theta, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the loss function and its gradient for softmax regression (vectorized version) \n",
    "Now complete the function **softmax\\_loss\\_vectorized** in **softmax.py** to implement the loss function $J(\\theta)$ without using any **for** loops. Re-express the computation in terms of matrix operations on $X$, $y$ and $\\theta$. \n",
    "Now vectorize the gradient computation in **softmax\\_loss\\_vectorized** in **softmax.py**. Once you complete this, the cell below will run and time your naive and vectorized implementations -- you should expect to see at least one order of magnitude difference in run time between the two implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "from softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(theta, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# We use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing mini-batch gradient descent \n",
    "In large-scale applications, the training data can have millions of examples. Hence, it seems wasteful to compute the  loss function over the entire training set in order to perform only a single parameter update. A very common approach to addressing this challenge is to compute the gradient over batches of the training data. For example, a typical batch contains 256 examples from a training set of over 1.2 million. This batch is then used to perform a parameter update:\n",
    "$$ \\theta^{(k)} \\to \\theta^{(k)} - \\alpha \\nabla_{\\theta^{(k)}} J(\\theta) $$\n",
    "where $\\alpha$ is the step size or learning rate for gradient descent.\n",
    "\n",
    "Implement mini-batch gradient descent in the method **train** in **linear_classifier.py** using the description provided in the documentation of the method. You can set the **verbose** argument of **train** to be **True** and observe how the loss function varies with iteration number.\n",
    "\n",
    "You will test your implementation of mini-batch gradient descent in the search for hyperparameters below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a validation set to select regularization lambda and learning rate for   gradient descent\n",
    "There are many hyper parameters to be selected for mini batch gradient descent -- the batch size, the number of iterations, and the learning rate. For the loss function, we also need to select $\\lambda$, the regularization strength. In this exercise, we have pre-selected a batch size of 400 and an iteration count of 4000. Now, use the validation set provided to sweep the learning rate and the $\\lambda$ parameter space, using the suggested values in the cell below to find the best combination of these two hyper parameters. Fill in the code to do this in the section marked **TODO** below.\n",
    "\n",
    "Once you find the best values of $\\lambda$ and learning rate, insert code in the cell below to train a softmax classifier on the training data with the best hyper parameters and save this classifier in the variable  **best\\_softmax**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set and the test set.\n",
    "\n",
    "import linear_classifier\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7, 1e-6, 5e-6]\n",
    "regularization_strengths = [5e4, 1e5, 5e5, 1e8]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# Save the best trained softmax classifer in variable best_softmax.            #\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the best softmax classifier on the test set and visualizing the coefficients\n",
    "The best\\_softmax classifier will be  evaluated on the set aside test set and you should expect to see an overall accuracy of over 35%. \n",
    "\n",
    "Compute the confusion matrix (you can use the confusion\\_matrix function in **sklearn.metrics** on the test set for your predictor and interpret the visualized coefficients in the light of the errors made by the classifier.\n",
    "\n",
    "We can remove the bias term from the $\\theta$ matrix and reshape each column of the matrix which is a parameter vector of size $3072$ back into an array of size $32\\times 32 \\times 3$ and visualize the results as an image. The cell below  constructs such a plot, similar to the one in Figure 2 in hw3.pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best softmax classifier on test set\n",
    "\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )\n",
    "\n",
    "# compute confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print confusion_matrix(y_test,y_test_pred)\n",
    "\n",
    "# Visualize the learned weights for each class`\n",
    "\n",
    "theta = best_softmax.theta[1:,:].T # strip out the bias term\n",
    "theta = theta.reshape(10, 32, 32, 3)\n",
    "\n",
    "theta_min, theta_max = np.min(theta), np.max(theta)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  thetaimg = 255.0 * (theta[i].squeeze() - theta_min) / (theta_max - theta_min)\n",
    "  plt.imshow(thetaimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extra credit:  Experimenting with other hyper parameters and optimization method\n",
    "We chose a batch size of 400 and 4000 iterations for our previous experiments. Explore larger and smaller batch sizes, choosing an appropriate number of iterations (by specifying a tolerance on differences in values of the loss function or its gradient in successive iterations) with the validation set. Produce plots that show the variation of test set accuracy as a function of batch size/number of iterations. You will have to determine the right settings for regularization strength $\\lambda$ and learning rate for each batch size/number of iterations combination. What is the best batch size/number of iterations/learning rate/regularization strength combination for this problem? What is the best test set accuracy that can be achieved by this combination? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
